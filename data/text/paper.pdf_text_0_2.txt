for 
approximate 
nearest-neighbor 
searches, 
these 
embeddings 
facilitate 
low-latency retrieval, enabling more effective anomaly 
detection, 
similarity-based 
grouping, 
and 
root-cause 
exploration [6], [7]. In this paper, we propose a general, 
extensible framework that (i) preprocesses and normalizes raw 
log streams, (ii) leverages a pre trained language model for 
embedding generation, and (iii) indexes embeddings in a 
vector database to support real-time similarity search, 
clustering, and anomaly detection. We validate our approach 
on multiple public benchmark datasets, demonstrating 
improvements of up to 15% in detection accuracy and 
reductions of up to 70% in query latency compared to 
traditional methods. 
II.​ RELATED WORK 
In [1], Guan et al. introduce LogLLM, a novel approach to 
log-based anomaly detection leveraging large language 
models fine-tuned on extensive, domain-agnostic log corpora. 
They preprocess raw logs via tokenization and normalization 
before feeding them into a transformer-based model that 
learns contextual embeddings of log events. The system then 
employs semantic similarity measures to distinguish normal 
from anomalous patterns. Their evaluation on multiple 
benchmark datasets demonstrates that LogLLM achieves high 
recall and precision, outperforming traditional keyword‐based 
detectors by over 12%. The work highlights the efficacy of 
language models in capturing nuanced syntactic and semantic