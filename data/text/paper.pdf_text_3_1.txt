analysis. 
Fig. 1 illustrates the end-to-end pipeline of our proposed log 
analysis framework, from ingestion to classification and 
feedback. Logs are first collected from diverse sources in the 
Log Ingestion phase and then passed through a Preprocessing 
stage for normalization, tokenization, and enrichment. The 
processed logs are encoded into semantic embeddings during 
the Embedding Generation step, which are subsequently 
stored and indexed in a Vector Database based on HNSW 
(Hierarchical 
Navigable 
Small 
World) 
graphs. These 
embeddings are queried during the Labeling & Classification 
phase using k-nearest-neighbor (k-NN) techniques to assign 
labels. Finally, a Feedback Loop captures corrections from 
human analysts or automated feedback signals, allowing the 
system 
to 
continuously 
refine 
its 
embeddings 
and 
classification thresholds. 
​
 
Fig 1. End-to-end pipeline for log data processing 
and classification 
E.​ Labeling, Classification, and Feedback 
k-NN Classification & Majority Vote: For each new log 
embedding, the k most similar vectors are retrieved. Their 
associated labels—such as anomaly types, error categories, or 
functional tags—are aggregated via majority vote. A 
similarity threshold ensures low-confidence cases are flagged 
as “UNKNOWN” for manual review. 
Clustering for Unsupervised Grouping: DBSCAN or 
HDBSCAN is periodically applied over recent embeddings to 
uncover emerging clusters of semantically related logs (e.g., a